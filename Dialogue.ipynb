{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "saving-chair",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from cleantext import clean\n",
    "from pandas import DataFrame\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fantastic-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-document",
   "metadata": {},
   "source": [
    "Vérification de la liste des fichiers, ce notebook nécessite au moins la présence de Data_train.csv Data_train_2.csv Label_train.csv Label_train_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incomplete-moderator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Label_train_2.csv', '.DS_Store', 'Creation_matrice_Creation.ipynb', 'requirements.txt', 'FinalDBelon.csv', 'Data_train.csv', 'DataNONEL_clean.csv', 'Clean_Tweets_EM.ipynb', 'Creation_matrice_Detection.ipynb', 'README.md', 'DataBase_temporary_creation.csv', 'Tweets_Not_ElonMusk.csv', 'DataBase.csv', '.gitignore', 'Dialogue.ipynb', 'Data_train_2.csv', 'DataElon_clean.csv', '.ipynb_checkpoints', 'Label_train.csv', 'Clean_Tweets_NONEM.ipynb', '.git', 'TweetsElonMusk.csv']\n"
     ]
    }
   ],
   "source": [
    "arr = os.listdir('.')\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-verse",
   "metadata": {},
   "source": [
    "Test si la version de tensor flow utilisée est bien 2.0, ne renvoie rien après son exécution si la bonne version est utilisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worldwide-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hasattr(tf, 'function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-philosophy",
   "metadata": {},
   "source": [
    " ### Importation des datasets d'entrainement.\n",
    "\\\n",
    "La partie Detection a pour dataset d'entrainement Data_train et ses labels Label_train.\n",
    "\\\n",
    "La partie Creation a pour dataset d'entrainement Data_train_2 et ses labels Label_train_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "optical-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train = pd.read_csv(\"Data_train.csv\")\n",
    "Label_train = pd.read_csv(\"Label_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "concerned-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train_2 = pd.read_csv('Data_train_2.csv')\n",
    "Label_train_2 = pd.read_csv(\"Label_train_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-address",
   "metadata": {},
   "source": [
    " ### Construction du modele utilisé pour les deux réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "selected-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Construction_modele(Data_train, Label_train, affichage=1):\n",
    "    #Construction couches de neurones\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))\n",
    "    #Mise au bon format des donnees pour le modele\n",
    "    Tweet_1 = Data_train.values\n",
    "    Tweet_2 = Tweet_1.astype(dtype='float32')\n",
    "    Label_1 = Label_train.values\n",
    "    Label_2 = Label_1[:,0].astype(dtype='uint8')\n",
    "    #Ajustement modele et choix de metrique\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "    #Entrainement du modele\n",
    "    model.fit(Tweet_2, Label_2, epochs=18, verbose=affichage)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-copyright",
   "metadata": {},
   "source": [
    " ### Fonction de vectorisation de tweet\n",
    " Prend un tweet en argument sous la forme d'une chaîne de caractères et renvoie son vecteur sous forme d'une database dans l'espace vectoriel de plongement utilisé par l'argument Data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interesting-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorisation(Data_train, tweet):  \n",
    "    Vecteurs = Data_train.columns\n",
    "    row = []\n",
    "    \n",
    "    dictionnary = dict()\n",
    "    for mot in Vecteurs:\n",
    "        dictionnary[mot] = 0\n",
    "    \n",
    "    Mot_tweet = tweet.split(\" \")\n",
    "    \n",
    "    for index, mot in enumerate(Mot_tweet):\n",
    "            if mot in Vecteurs:\n",
    "                dictionnary[mot] = index + 1\n",
    "                \n",
    "    row.append(dictionnary)\n",
    "    Data_out = DataFrame(row)\n",
    "    \n",
    "    if Data_out.loc[0].max() != 0:\n",
    "        Data_out.loc[0] = Data_out.loc[0] / Data_out.loc[0].max()\n",
    "    \n",
    "    return Data_out\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-necklace",
   "metadata": {},
   "source": [
    " ### Fonction de normalisation d'un vecteur \n",
    "Le vecteur est normalisé par rapport à la norme infinie $||vecteur||_{\\infty} = 1$ ou $0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anticipated-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(x):\n",
    "    if x.max() != 0:\n",
    "        x = x / x.max()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-history",
   "metadata": {},
   "source": [
    " ### Fonction de vectorisation d'une liste de tweets\n",
    " Semblable à vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "magnetic-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorisation_liste(Data_train, liste_tweet, func_norma = normalisation):   \n",
    "    Vecteurs = Data_train.columns\n",
    "    \n",
    "    row = []\n",
    "    \n",
    "    dictionnary = dict()\n",
    "    for mot in Vecteurs:\n",
    "        dictionnary[mot] = 0\n",
    "    \n",
    "    for tweet in liste_tweet:\n",
    "        \n",
    "        Mot_tweet = tweet.split(\" \")\n",
    "        dictionnary2 = dictionnary.copy()\n",
    "        for index, mot in enumerate(Mot_tweet):\n",
    "            if mot in Vecteurs:\n",
    "                dictionnary2[mot] = index + 1\n",
    "                \n",
    "        row.append(dictionnary2)\n",
    "        \n",
    "    Data_out = DataFrame(row)\n",
    "    Data_out = Data_out.apply(normalisation, axis=1)\n",
    "        \n",
    "    return Data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-pollution",
   "metadata": {},
   "source": [
    " ### Fonction de détection d'un tweet\n",
    "La fonction de détection prend en argument un tweet sous la forme d'une chaîne de caractères. Le réseau de neurones entrainé passé en argument renvoie son évaluation du tweet, sous la forme d'un tuple :\n",
    "\\\n",
    "\\\n",
    "(\"Elon Musk detecte a x%\", 1)\n",
    "\\\n",
    "(\"Indetermine\", 2)\n",
    "\\\n",
    "(\"Pas Elon Musk a x%\", 0)\n",
    "\\\n",
    "\\\n",
    "Si la probabilité selon le réseau que le tweet appartienne à Elon Musk est supérieure à 60%, le tweet sera considéré comme appartenant à Elon Musk, si la probabilité que le tweet appartienne à Elon Musk est inférieure à 40%, le tweet sera considéré comme n'appartenant pas à Elon Musk. Si la probabilité est comprise entre 40% et 60%, le tweet sera considéré comme indéterminé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "regular-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(tweet_sentence, model_neurone, Data_train):\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    vecteur = vecteur_phrase.values\n",
    "    vecteur = vecteur.astype(dtype='float32')\n",
    "    if vecteur.sum() == 0:\n",
    "        char = \"Indetermine\"\n",
    "        return (char, 2)\n",
    "    Prediction = model_neurone.predict(vecteur)\n",
    "    if Prediction[0, 1] >= 0.6:\n",
    "        char = \"Elon Musk detecte a \" + str(Prediction[0, 1]) + \"%\"\n",
    "        return (char, 1)\n",
    "    elif Prediction[0,1] >= 0.4:\n",
    "        char = \"Indetermine\"\n",
    "        return (char, 2)\n",
    "    else :\n",
    "        char = \"Pas Elon Musk a \" + str(Prediction[0, 0]) + \"%\"\n",
    "        return (char, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-cradle",
   "metadata": {},
   "source": [
    " ### Fonction de détection d'un tweet dans une forme brut\n",
    " Renvoie l'évaluation d'un tweet par le réseau de neurones entrainé de manière brute i.e. la probabilité que ce soit un tweet de Elon Musk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "administrative-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_creation(tweet_sentence, model_neurone, Data_train):\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    vecteur = vecteur_phrase.values\n",
    "    vecteur = vecteur.astype(dtype='float32')\n",
    "    if vecteur.sum() == 0:\n",
    "        return 0\n",
    "    Prediction = model_neurone.predict(vecteur)\n",
    "    return Prediction[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-infrared",
   "metadata": {},
   "source": [
    "Exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "independent-teach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92967653"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#detection_creation(\"Spacex is the futur of humanity, and space will be our home\", reseau_neurone_creation, Data_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-memorial",
   "metadata": {},
   "source": [
    " ### Fonction de détection et d'apprentissage\n",
    "La fonction détecte et rend une estimation du tweet proposé et l'ajoute à son entrainement\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "useful-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_apprentissage(tweet_sentence, model_neurone, Data_train, Label_train, val=0):\n",
    "    #estimation\n",
    "    (char, val) = detection(tweet_sentence, model_neurone, Data_train)\n",
    "    #re entrainement du reseau de neurones\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    Data_train = pd.concat([Data_train, vecteur_phrase], ignore_index=True)\n",
    "    Data_train.reset_index(drop=True, inplace=True)\n",
    "    label = DataFrame([{'label':val}])\n",
    "    Label_train = pd.concat([Label_train, label], ignore_index=True)\n",
    "    Label_train.reset_index(drop=True, inplace=True)\n",
    "    model = Construction_modele(Data_train, Label_train, affichage=0)\n",
    "    return ((char, val), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-saudi",
   "metadata": {},
   "source": [
    "### Fonction de détection et d'apprentissage\n",
    "La fonction de détection et d'apprentissage d'une liste prend en argument une liste de tweet sous la forme [\"tweet1\", \"tweet2\"…] et les labels associés sous la forme d'une liste [0,0,1…]. La fonction renvoie une liste [model_neurone, Data, Label, tweet_liste, valeur]\n",
    "\\\n",
    "$ \\textbf{model_neurone} $ est le réseau de neurones réentrainé avec la liste de tweet fournie et aux labels indiqués en entrée.\n",
    "\\\n",
    "$ \\textbf{Data} $ et $ \\textbf{Label} $ sont les nouveaux datasets d'entrainement du réseau, qui sont ceux passés en arguments et concaténés avec les nouveaux\n",
    "\\\n",
    "$ \\textbf{tweet_liste} $ est la liste de tweets initiaux passée en argument\n",
    "\\\n",
    "$ \\textbf{valeur} $ est une liste des estimations évaluées avant ré-apprentissage du réseau de neurones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "retired-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_apprentissage_liste(tweet_liste, model_neurone, Data_train, Label_train, val_list):\n",
    "    Estimat = []\n",
    "    Data_adding = Vectorisation_liste(Data_train, tweet_liste)\n",
    "    Data = Data_train.copy()\n",
    "    \n",
    "    Label_adding = pd.DataFrame(val_list, columns=['label'])\n",
    "    Label = Label_train.copy()\n",
    "    \n",
    "    Data_add = Data_adding.values\n",
    "    Data_add = Data_add.astype(dtype='float32')\n",
    "    \n",
    "    valued = model_neurone.predict(Data_add)\n",
    "    \n",
    "    Data = pd.concat([Data, Data_adding], ignore_index = True)\n",
    "    Label = pd.concat([Label, Label_adding], ignore_index = True)\n",
    "    \n",
    "    model_neurone = Construction_modele(Data, Label, affichage=0)\n",
    "    \n",
    "    valeur = (valued[:,1] > 0.5).astype(dtype='int').tolist()\n",
    "\n",
    "    \n",
    "    return [model_neurone, Data, Label, tweet_liste, valeur]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-indian",
   "metadata": {},
   "source": [
    "### Fonction de création d'un tweet\n",
    "La fonction a pour but de créer un tweet d'une longueur attendue, et renvoie un tweet que le réseau de neurones juge soit acceptable (à une estimation supérieure à 0.9) soit a essayé d'atteindre ce niveau (estimation supérieure à 0.9) 100 fois (sans réussir).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "printable-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Creation_tweet(nombre_mots_total, phrase, Data_train_2, model_neurone): \n",
    "    phrase = phrase.lower()\n",
    "    phrase = clean(phrase, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    liste_mot =  phrase.split(\" \")\n",
    "    if nombre_mots_total < len(liste_mot):\n",
    "        print(\"Erreur liste mot plus longue que le tweet prevu, longueur du tweet pris par defaut au nombre de mots\")\n",
    "        nombre_mots_total = len(liste_mot)\n",
    "    nombre_mot_rest = nombre_mots_total - len(liste_mot)\n",
    "    \n",
    "    liste_tweet = liste_mot + rnd.sample(list(Data_train_2.columns), nombre_mot_rest, counts=None)\n",
    "    tweet_pertinent = ' '.join(liste_tweet)\n",
    "    pertinence = detection_creation(tweet_pertinent, model_neurone, Data_train_2)\n",
    "    \n",
    "    for iteration in range(100):\n",
    "        if pertinence > 0.9:\n",
    "            tweet_pertinent = clean(tweet_pertinent, no_punct=True)\n",
    "            return tweet_pertinent\n",
    "        liste = liste_mot + rnd.sample(list(Data_train_2.columns), nombre_mot_rest, counts=None)\n",
    "        rnd.shuffle(liste)\n",
    "        current_tweet = ' '.join(liste)\n",
    "        current_pertinence = detection_creation(current_tweet, model_neurone, Data_train_2)\n",
    "        if current_pertinence > pertinence:\n",
    "            pertinence = current_pertinence\n",
    "            tweet_pertinent = current_tweet\n",
    "    current = clean(tweet_pertinent, no_punct=True)\n",
    "    return current\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-thumb",
   "metadata": {},
   "source": [
    "# Initialisation des réseaux de neurones avant le dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reliable-silence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.6869 - accuracy: 0.5757\n",
      "Epoch 2/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.6697 - accuracy: 0.6813\n",
      "Epoch 3/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.6429 - accuracy: 0.7255\n",
      "Epoch 4/18\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.5986 - accuracy: 0.7565\n",
      "Epoch 5/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7803\n",
      "Epoch 6/18\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.4776 - accuracy: 0.7987\n",
      "Epoch 7/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4279 - accuracy: 0.8186\n",
      "Epoch 8/18\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.3908 - accuracy: 0.8339\n",
      "Epoch 9/18\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.3633 - accuracy: 0.8440\n",
      "Epoch 10/18\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.3419 - accuracy: 0.8518\n",
      "Epoch 11/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3253 - accuracy: 0.8578\n",
      "Epoch 12/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3106 - accuracy: 0.8663\n",
      "Epoch 13/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2986 - accuracy: 0.8717\n",
      "Epoch 14/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2881 - accuracy: 0.8738\n",
      "Epoch 15/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2797 - accuracy: 0.8782\n",
      "Epoch 16/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2716 - accuracy: 0.8817\n",
      "Epoch 17/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2648 - accuracy: 0.8846\n",
      "Epoch 18/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2585 - accuracy: 0.8863\n"
     ]
    }
   ],
   "source": [
    "reseau_neurone_detection = Construction_modele(Data_train, Label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "overall-exposure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6883 - accuracy: 0.5476\n",
      "Epoch 2/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6788 - accuracy: 0.6109\n",
      "Epoch 3/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6650 - accuracy: 0.6534\n",
      "Epoch 4/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6440 - accuracy: 0.6867\n",
      "Epoch 5/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.6152 - accuracy: 0.7113\n",
      "Epoch 6/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.5809 - accuracy: 0.7318\n",
      "Epoch 7/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.5467 - accuracy: 0.7505\n",
      "Epoch 8/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.5164 - accuracy: 0.7661\n",
      "Epoch 9/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.4902 - accuracy: 0.7794\n",
      "Epoch 10/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.4674 - accuracy: 0.7928\n",
      "Epoch 11/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.4482 - accuracy: 0.8010\n",
      "Epoch 12/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.4313 - accuracy: 0.8080\n",
      "Epoch 13/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.4165 - accuracy: 0.8177\n",
      "Epoch 14/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.4042 - accuracy: 0.8249\n",
      "Epoch 15/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3934 - accuracy: 0.8284\n",
      "Epoch 16/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3841 - accuracy: 0.8340\n",
      "Epoch 17/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3761 - accuracy: 0.8380\n",
      "Epoch 18/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3679 - accuracy: 0.8389\n"
     ]
    }
   ],
   "source": [
    "reseau_neurone_creation = Construction_modele(Data_train_2, Label_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-thomas",
   "metadata": {},
   "source": [
    "# Début du dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pharmaceutical-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train_Detection = Data_train.copy()\n",
    "Data_train_Creation = Data_train_2.copy()\n",
    "\n",
    "Label_train_Detection = Label_train.copy()\n",
    "Label_train_Creation = Label_train_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "disciplinary-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue(iteration, nombre_mot ,reseau_neurone_detection, reseau_neurone_creation, Data_train, Data_train_2, Label_train, Label_train_2):\n",
    "    nombre_mot += 1\n",
    "    evolution = []\n",
    "    for i in range(iteration):\n",
    "        \n",
    "        #Creation parle en premier et propose 30 tweets, qu il juge pertinent, a Detection\n",
    "        print('Tweet propose par creation a la ', i , 'eme iteration : \\n')\n",
    "        tweet_liste = []\n",
    "        valeur = [0]*30 \n",
    "        for j in range(30):\n",
    "            \n",
    "            tweet_proposal = Creation_tweet(nombre_mot, '', Data_train_2, reseau_neurone_creation)\n",
    "            print(tweet_proposal)\n",
    "            tweet_liste.append(tweet_proposal)\n",
    "        print('\\n')\n",
    "        \n",
    "        #Detection recoit ces 30 tweets, les evalues, les appreds en tenant compte qu ils sont faux, et retourne a creation ce qu il en pensait avant d apprendre qu ils etaient faux\n",
    "        \n",
    "        [reseau_neurone_detection, Data_train, Label_train, tweet_liste, evaluation] = detection_apprentissage_liste(tweet_liste, reseau_neurone_detection, Data_train, Label_train, valeur)\n",
    "        \n",
    "        #Affichage de ce que globalement detection en pensait\n",
    "        \n",
    "        print('Sur ces tweets propose detection en a trouve :', sum(evaluation), ' correct selon lui')\n",
    "        \n",
    "        evolution.append(sum(evaluation))\n",
    "        \n",
    "        #Creation apprend de ses tweets en les labelisant suivant ce que detection en avait pense a priori\n",
    "        \n",
    "        [reseau_neurone_creation, Data_train_2, Label_train_2, _, _] = detection_apprentissage_liste(tweet_liste, reseau_neurone_creation, Data_train_2, Label_train_2, evaluation)\n",
    "        \n",
    "        print('\\n \\n')\n",
    "        \n",
    "    return [evolution, reseau_neurone_detection, reseau_neurone_creation, Data_train, Data_train_2, Label_train, Label_train_2]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dried-eagle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet propose par creation a la  0 eme iteration : \n",
      "\n",
      "so st short tube tune > both\n",
      "carpet line obama found free slow release\n",
      "posted really final exactly youve die way\n",
      "stop using matters mph also hell complete\n",
      "kids covered but media news water ground\n",
      "jobs solar clear nasa tiny model cute\n",
      "awesome inside fair using navy definitely finally\n",
      "grateful wonderful turn tesla matters video btw\n",
      "met internet worked thursday record home california\n",
      "yes future orbit here autopilot ship use\n",
      "nasa all art starting birthday wrong definitely\n",
      "forward want nasa built up guy landing\n",
      "gets pressure photo years model supercharger rate\n",
      "camera going price thank free landing into\n",
      "turn booster hw boring build come data\n",
      "mean congratulations join thrust end drive matters\n",
      "york vote barbados jobs hyperloop car will\n",
      "rio sounds awesome using fully hair course\n",
      "spacex few person second via love station\n",
      "until friend step quality about exciting landing\n",
      "cheek free makes month really matter very\n",
      "foundation thats via station think definitely youtube\n",
      "doing support course california beauty awesome black\n",
      "am be hot yeah these congratulations mars\n",
      "ready added before improve point having care\n",
      "at catch flying nature saw under unveil\n",
      "getcovered near turn cover cause lot march\n",
      "vote your bitch fine enough a falcon\n",
      "came pass sense he really check after\n",
      "jazz is everything says hold climate shows\n",
      "\n",
      "\n",
      "Sur ces tweets propose detection en a trouve : 28  correct selon lui\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialogue_summary = dialogue(1, 7, reseau_neurone_detection, reseau_neurone_creation, Data_train_Detection, Data_train_Creation, Label_train_Detection, Label_train_Creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-philosophy",
   "metadata": {},
   "source": [
    "### Tests unitaires \n",
    "Tests unitaires de certaines fonctions clés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-theater",
   "metadata": {},
   "source": [
    "Ce test unitaire à pour but de vérifier que la fonction Vectorisation renvoie un bien une DataFrame lorsqu'elle prend une phrase quelconque en argument. Le test verifiera aussi que les colonnes du DataFrame crée corresponde avec celui de Data_train passé en argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tested-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "complex-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unitaire_vectorisation():\n",
    "    tweet = str(uuid.uuid4())\n",
    "    tweet = tweet.replace('-', ' ')\n",
    "    test_list = np.random.randint(10, size=(1,3))\n",
    "    Data_testunitaire = pd.DataFrame(test_list, columns=['column1', 'column2', 'column3'])\n",
    "    for i in range(10):\n",
    "        vectorisation = Vectorisation(Data_testunitaire, tweet)\n",
    "        if vectorisation.shape != Data_testunitaire.shape:\n",
    "            return False\n",
    "        if not (vectorisation.columns == Data_testunitaire.columns).all():\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "terminal-front",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_unitaire_vectorisation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
