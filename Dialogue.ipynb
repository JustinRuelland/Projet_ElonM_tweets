{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "saving-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from cleantext import clean\n",
    "from pandas import DataFrame\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "graduate-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-document",
   "metadata": {},
   "source": [
    "Vérification de la liste des fichiers, ce notebook nécessite au moins la présence de Data_train.csv Data_train_2.csv Label_train.csv Label_train_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "incomplete-moderator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Label_train_2.csv', '.DS_Store', 'Creation_matrice_Creation.ipynb', 'FinalDBelon.csv', 'Data_train.csv', 'DataNONEL_clean.csv', 'Clean_Tweets_EM.ipynb', 'Creation_matrice_Detection.ipynb', 'README.md', 'DataBase_temporary_creation.csv', 'Tweets_Not_ElonMusk.csv', 'DataBase.csv', '.gitignore', 'Reseau_neurone.ipynb', 'Data_train_2.csv', 'DataElon_clean.csv', '.ipynb_checkpoints', 'Label_train.csv', 'Clean_Tweets_NONEM.ipynb', '.git', 'TweetsElonMusk.csv']\n"
     ]
    }
   ],
   "source": [
    "arr = os.listdir('.')\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-verse",
   "metadata": {},
   "source": [
    "Test si la version de tensor flow utilisée est bien 2.0, ne renvoie rien après son exécution si la bonne version est utilisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "worldwide-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hasattr(tf, 'function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-philosophy",
   "metadata": {},
   "source": [
    " ### Importation des datasets d'entrainement.\n",
    "\\\n",
    "La partie Detection a pour dataset d'entrainement Data_train et ses labels Label_train.\n",
    "\\\n",
    "La partie Creation a pour dataset d'entrainement Data_train_2 et ses labels Label_train_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "optical-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train = pd.read_csv(\"Data_train.csv\")\n",
    "Label_train = pd.read_csv(\"Label_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "concerned-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train_2 = pd.read_csv('Data_train_2.csv')\n",
    "Label_train_2 = pd.read_csv(\"Label_train_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-address",
   "metadata": {},
   "source": [
    " ### Construction du modele utilisé pour les deux réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "selected-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Construction_modele(Data_train, Label_train, affichage=1):\n",
    "    #Construction couches de neurones\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))\n",
    "    #Mise au bon format des donnees pour le modele\n",
    "    Tweet_1 = Data_train.values\n",
    "    Tweet_2 = Tweet_1.astype(dtype='float32')\n",
    "    Label_1 = Label_train.values\n",
    "    Label_2 = Label_1[:,0].astype(dtype='uint8')\n",
    "    #Ajustement modele et choix de metrique\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "    #Entrainement du modele\n",
    "    model.fit(Tweet_2, Label_2, epochs=18, verbose=affichage)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-copyright",
   "metadata": {},
   "source": [
    " ### Fonction de vectorisation de tweet\n",
    " Prend un tweet en argument sous la forme d'une chaîne de caractères et renvoie son vecteur sous forme d'une database dans l'espace vectoriel de plongement utilisé par l'argument Data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "interesting-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorisation(Data_train, tweet):  \n",
    "    Vecteurs = Data_train.columns\n",
    "    row = []\n",
    "    \n",
    "    dictionnary = dict()\n",
    "    for mot in Vecteurs:\n",
    "        dictionnary[mot] = 0\n",
    "    \n",
    "    Mot_tweet = tweet.split(\" \")\n",
    "    \n",
    "    for index, mot in enumerate(Mot_tweet):\n",
    "            if mot in Vecteurs:\n",
    "                dictionnary[mot] = index + 1\n",
    "                \n",
    "    row.append(dictionnary)\n",
    "    Data_out = DataFrame(row)\n",
    "    \n",
    "    if Data_out.loc[0].max() != 0:\n",
    "        Data_out.loc[0] = Data_out.loc[0] / Data_out.loc[0].max()\n",
    "    \n",
    "    return Data_out\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-necklace",
   "metadata": {},
   "source": [
    " ### Fonction de normalisation d'un vecteur \n",
    "Le vecteur est normalisé par rapport à la norme infinie $||vecteur||_{\\infty} = 1$ ou $0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "anticipated-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(x):\n",
    "    if x.max() != 0:\n",
    "        x = x / x.max()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-history",
   "metadata": {},
   "source": [
    " ### Fonction de vectorisation d'une liste de tweets\n",
    " Semblable à vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "magnetic-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorisation_liste(Data_train, liste_tweet, func_norma = normalisation):   \n",
    "    Vecteurs = Data_train.columns\n",
    "    \n",
    "    row = []\n",
    "    \n",
    "    dictionnary = dict()\n",
    "    for mot in Vecteurs:\n",
    "        dictionnary[mot] = 0\n",
    "    \n",
    "    for tweet in liste_tweet:\n",
    "        \n",
    "        Mot_tweet = tweet.split(\" \")\n",
    "        dictionnary2 = dictionnary.copy()\n",
    "        for index, mot in enumerate(Mot_tweet):\n",
    "            if mot in Vecteurs:\n",
    "                dictionnary2[mot] = index + 1\n",
    "                \n",
    "        row.append(dictionnary2)\n",
    "        \n",
    "    Data_out = DataFrame(row)\n",
    "    Data_out = Data_out.apply(normalisation, axis=1)\n",
    "        \n",
    "    return Data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-pollution",
   "metadata": {},
   "source": [
    " ### Fonction de détection d'un tweet\n",
    "La fonction de détection prend en argument un tweet sous la forme d'une chaîne de caractères. Le réseau de neurones entrainé passé en argument renvoie son évaluation du tweet, sous la forme d'un tuple :\n",
    "\\\n",
    "\\\n",
    "(\"Elon Musk detecte a x%\", 1)\n",
    "\\\n",
    "(\"Indetermine\", 2)\n",
    "\\\n",
    "(\"Pas Elon Musk a x%\", 0)\n",
    "\\\n",
    "\\\n",
    "Si la probabilité selon le réseau que le tweet appartienne à Elon Musk est supérieure à 60%, le tweet sera considéré comme appartenant à Elon Musk, si la probabilité que le tweet appartienne à Elon Musk est inférieure à 40%, le tweet sera considéré comme n'appartenant pas à Elon Musk. Si la probabilité est comprise entre 40% et 60%, le tweet sera considéré comme indéterminé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "regular-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(tweet_sentence, model_neurone, Data_train):\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    vecteur = vecteur_phrase.values\n",
    "    vecteur = vecteur.astype(dtype='float32')\n",
    "    if vecteur.sum() == 0:\n",
    "        char = \"Indetermine\"\n",
    "        return (char, 2)\n",
    "    Prediction = model_neurone.predict(vecteur)\n",
    "    if Prediction[0, 1] >= 0.6:\n",
    "        char = \"Elon Musk detecte a \" + str(Prediction[0, 1]) + \"%\"\n",
    "        return (char, 1)\n",
    "    elif Prediction[0,1] >= 0.4:\n",
    "        char = \"Indetermine\"\n",
    "        return (char, 2)\n",
    "    else :\n",
    "        char = \"Pas Elon Musk a \" + str(Prediction[0, 0]) + \"%\"\n",
    "        return (char, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-cradle",
   "metadata": {},
   "source": [
    " ### Fonction de détection d'un tweet dans une forme brut\n",
    " Renvoie l'évaluation d'un tweet par le réseau de neurones entrainé de manière brute i.e. la probabilité que ce soit un tweet de Elon Musk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "administrative-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_creation(tweet_sentence, model_neurone, Data_train):\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    vecteur = vecteur_phrase.values\n",
    "    vecteur = vecteur.astype(dtype='float32')\n",
    "    if vecteur.sum() == 0:\n",
    "        return 0\n",
    "    Prediction = model_neurone.predict(vecteur)\n",
    "    return Prediction[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-infrared",
   "metadata": {},
   "source": [
    "Exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "independent-teach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94794744"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_creation(\"Spacex is the futur of humanity, and space will be our home\", reseau_neurone_creation, Data_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-memorial",
   "metadata": {},
   "source": [
    " ### Fonction de détection et d'apprentissage\n",
    "La fonction détecte et rend une estimation du tweet proposé et l'ajoute à son entrainement\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "useful-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_apprentissage(tweet_sentence, model_neurone, Data_train, Label_train, val=0):\n",
    "    #estimation\n",
    "    (char, val) = detection(tweet_sentence, model_neurone, Data_train)\n",
    "    #re entrainement du reseau de neurones\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    Data_train = pd.concat([Data_train, vecteur_phrase], ignore_index=True)\n",
    "    Data_train.reset_index(drop=True, inplace=True)\n",
    "    label = DataFrame([{'label':val}])\n",
    "    Label_train = pd.concat([Label_train, label], ignore_index=True)\n",
    "    Label_train.reset_index(drop=True, inplace=True)\n",
    "    model = Construction_modele(Data_train, Label_train, affichage=0)\n",
    "    return ((char, val), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-saudi",
   "metadata": {},
   "source": [
    "### Fonction de détection et d'apprentissage\n",
    "La fonction de détection et d'apprentissage d'une liste prend en argument une liste de tweet sous la forme [\"tweet1\", \"tweet2\"…] et les labels associés sous la forme d'une liste [0,0,1…]. La fonction renvoie une liste [model_neurone, Data, Label, tweet_liste, valeur]\n",
    "\\\n",
    "$ \\textbf{model_neurone} $ est le réseau de neurones réentrainé avec la liste de tweet fournie et aux labels indiqués en entrée.\n",
    "\\\n",
    "$ \\textbf{Data} $ et $ \\textbf{Label} $ sont les nouveaux datasets d'entrainement du réseau, qui sont ceux passés en arguments et concaténés avec les nouveaux\n",
    "\\\n",
    "$ \\textbf{tweet_liste} $ est la liste de tweets initiaux passée en argument\n",
    "\\\n",
    "$ \\textbf{valeur} $ est une liste des estimations évaluées avant ré-apprentissage du réseau de neurones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "retired-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_apprentissage_liste(tweet_liste, model_neurone, Data_train, Label_train, val_list):\n",
    "    Estimat = []\n",
    "    Data_adding = Vectorisation_liste(Data_train, tweet_liste)\n",
    "    Data = Data_train.copy()\n",
    "    \n",
    "    Label_adding = pd.DataFrame(val_list, columns=['label'])\n",
    "    Label = Label_train.copy()\n",
    "    \n",
    "    Data_add = Data_adding.values\n",
    "    Data_add = Data_add.astype(dtype='float32')\n",
    "    \n",
    "    valued = model_neurone.predict(Data_add)\n",
    "    \n",
    "    Data = pd.concat([Data, Data_adding], ignore_index = True)\n",
    "    Label = pd.concat([Label, Label_adding], ignore_index = True)\n",
    "    \n",
    "    model_neurone = Construction_modele(Data, Label, affichage=0)\n",
    "    \n",
    "    valeur = (valued[:,1] > 0.5).astype(dtype='int').tolist()\n",
    "\n",
    "    \n",
    "    return [model_neurone, Data, Label, tweet_liste, valeur]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-indian",
   "metadata": {},
   "source": [
    "### Fonction de création d'un tweet\n",
    "La fonction a pour but de créer un tweet d'une longueur attendue, et renvoie un tweet que le réseau de neurones juge soit acceptable (à une estimation supérieure à 0.9) soit a essayé d'atteindre ce niveau (estimation supérieure à 0.9) 100 fois (sans réussir).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "printable-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Creation_tweet(nombre_mots_total, phrase, Data_train_2, model_neurone): \n",
    "    phrase = phrase.lower()\n",
    "    phrase = clean(phrase, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    liste_mot =  phrase.split(\" \")\n",
    "    if nombre_mots_total < len(liste_mot):\n",
    "        print(\"Erreur liste mot plus longue que le tweet prevu, longueur du tweet pris par defaut au nombre de mots\")\n",
    "        nombre_mots_total = len(liste_mot)\n",
    "    nombre_mot_rest = nombre_mots_total - len(liste_mot)\n",
    "    \n",
    "    liste_tweet = liste_mot + rnd.sample(list(Data_train_2.columns), nombre_mot_rest, counts=None)\n",
    "    tweet_pertinent = ' '.join(liste_tweet)\n",
    "    pertinence = detection_creation(tweet_pertinent, model_neurone, Data_train_2)\n",
    "    \n",
    "    for iteration in range(100):\n",
    "        if pertinence > 0.9:\n",
    "            tweet_pertinent = clean(tweet_pertinent, no_punct=True)\n",
    "            return tweet_pertinent\n",
    "        liste = liste_mot + rnd.sample(list(Data_train_2.columns), nombre_mot_rest, counts=None)\n",
    "        rnd.shuffle(liste)\n",
    "        current_tweet = ' '.join(liste)\n",
    "        current_pertinence = detection_creation(current_tweet, model_neurone, Data_train_2)\n",
    "        if current_pertinence > pertinence:\n",
    "            pertinence = current_pertinence\n",
    "            tweet_pertinent = current_tweet\n",
    "    current = clean(tweet_pertinent, no_punct=True)\n",
    "    return current\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-thumb",
   "metadata": {},
   "source": [
    "# Initialisation des réseaux de neurones avant le dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "reliable-silence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.6897 - accuracy: 0.5507\n",
      "Epoch 2/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.6762 - accuracy: 0.6226\n",
      "Epoch 3/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.6542 - accuracy: 0.7124\n",
      "Epoch 4/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.6156 - accuracy: 0.7490\n",
      "Epoch 5/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7697\n",
      "Epoch 6/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4989 - accuracy: 0.7890\n",
      "Epoch 7/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4476 - accuracy: 0.8051\n",
      "Epoch 8/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4074 - accuracy: 0.8247\n",
      "Epoch 9/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3770 - accuracy: 0.8380\n",
      "Epoch 10/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3532 - accuracy: 0.8483\n",
      "Epoch 11/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3346 - accuracy: 0.8574\n",
      "Epoch 12/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.8611\n",
      "Epoch 13/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3053 - accuracy: 0.8689\n",
      "Epoch 14/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2942 - accuracy: 0.8742\n",
      "Epoch 15/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.8765\n",
      "Epoch 16/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2756 - accuracy: 0.8819\n",
      "Epoch 17/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2679 - accuracy: 0.8855\n",
      "Epoch 18/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2609 - accuracy: 0.8887\n"
     ]
    }
   ],
   "source": [
    "reseau_neurone_detection = Construction_modele(Data_train, Label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "overall-exposure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.6866 - accuracy: 0.5578\n",
      "Epoch 2/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.6753 - accuracy: 0.6158\n",
      "Epoch 3/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.6598 - accuracy: 0.6706\n",
      "Epoch 4/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.6364 - accuracy: 0.6988\n",
      "Epoch 5/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.6052 - accuracy: 0.7147\n",
      "Epoch 6/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.7362\n",
      "Epoch 7/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.5376 - accuracy: 0.7548\n",
      "Epoch 8/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.5083 - accuracy: 0.7695\n",
      "Epoch 9/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.7820\n",
      "Epoch 10/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.7938\n",
      "Epoch 11/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.4424 - accuracy: 0.8048\n",
      "Epoch 12/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.4257 - accuracy: 0.8133\n",
      "Epoch 13/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.4111 - accuracy: 0.8215\n",
      "Epoch 14/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3990 - accuracy: 0.8283\n",
      "Epoch 15/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3879 - accuracy: 0.8332\n",
      "Epoch 16/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8339\n",
      "Epoch 17/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.3705 - accuracy: 0.8389\n",
      "Epoch 18/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.3637 - accuracy: 0.8433\n"
     ]
    }
   ],
   "source": [
    "reseau_neurone_creation = Construction_modele(Data_train_2, Label_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-thomas",
   "metadata": {},
   "source": [
    "# Début du dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "pharmaceutical-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train_Detection = Data_train.copy()\n",
    "Data_train_Creation = Data_train_2.copy()\n",
    "\n",
    "Label_train_Detection = Label_train.copy()\n",
    "Label_train_Creation = Label_train_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "disciplinary-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue(iteration, nombre_mot ,reseau_neurone_detection, reseau_neurone_creation, Data_train, Data_train_2, Label_train, Label_train_2):\n",
    "    nombre_mot += 1\n",
    "    for i in range(iteration):\n",
    "        \n",
    "        #Creation parle en premier et propose 30 tweets, qu il juge pertinent, a Detection\n",
    "        print('Tweet propose par creation a la ', i , 'eme iteration : \\n')\n",
    "        tweet_liste = []\n",
    "        valeur = [0]*30 \n",
    "        for j in range(30):\n",
    "            \n",
    "            tweet_proposal = Creation_tweet(nombre_mot, '', Data_train_2, reseau_neurone_creation)\n",
    "            print(tweet_proposal)\n",
    "            tweet_liste.append(tweet_proposal)\n",
    "        print('\\n')\n",
    "        \n",
    "        #Detection recoit ces 30 tweets, les evalues, les appreds en tenant compte qu ils sont faux, et retourne a creation ce qu il en pensait avant d apprendre qu ils etaient faux\n",
    "        \n",
    "        [reseau_neurone_detection, Data_train, Label_train, tweet_liste, evaluation] = detection_apprentissage_liste(tweet_liste, reseau_neurone_detection, Data_train, Label_train, valeur)\n",
    "        \n",
    "        #Affichage de ce que globalement detection en pensait\n",
    "        \n",
    "        print('Sur ces tweets propose detection en a trouve :', sum(evaluation), ' correct selon lui')\n",
    "        \n",
    "        #Creation apprend de ses tweets en les labelisant suivant ce que detection en avait pense a priori\n",
    "        \n",
    "        [reseau_neurone_creation, Data_train_2, Label_train_2, _, _] = detection_apprentissage_liste(tweet_liste, reseau_neurone_creation, Data_train_2, Label_train_2, evaluation)\n",
    "        \n",
    "        print('\\n \\n')\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dried-eagle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet propose par creation a la  0 eme iteration : \n",
      "\n",
      "mean super launch exciting the before center\n",
      "everything might tube where quality date rate\n",
      "record said hold aiming burn than bad\n",
      "finally landing critical yeah his light yes\n",
      "he after premiere zero few diamonds great\n",
      "best coverage media note pay ai version\n",
      "will support our ytredoriginals system available boring\n",
      "building real course tweets use single not\n",
      "close these on worth artrave took high\n",
      "stop booster tunnel straight tell climate yes\n",
      "artpop couldnt lot left th best asked\n",
      "thing glad rest major got pay should\n",
      "performance beauty below t should country ship\n",
      "seriously ok tried yeah cape guys being\n",
      "will advanced youtube water mins production r\n",
      "tesla taking less used videos across version\n",
      "say landing talking full york crew enough\n",
      "reality drive yet across level because heavy\n",
      "mr view videos mission yes as just\n",
      "unapologetic california rd red definitely job deep\n",
      "few this americans took humanity his truck\n",
      "my lucky families lot wage much companies\n",
      "week sent everyone now fight came tesla\n",
      "right many target model truck march tell\n",
      "online secret difference been front may short\n",
      "life run due while supercharger house free\n",
      "cheektocheek let short orbit games broken congress\n",
      "et single later flying company mins needs\n",
      "gas watch definitely awesome holiday bill president\n",
      "spaceship seems put from space test all\n",
      "\n",
      "\n",
      "Sur ces tweets propose detection en a trouve : 28  correct selon lui\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialogue(1, 7, reseau_neurone_detection, reseau_neurone_creation, Data_train_Detection, Data_train_Creation, Label_train_Detection, Label_train_Creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-montgomery",
   "metadata": {},
   "source": [
    "### Tests unitaires \n",
    "Tests unitaires de certaines fonctions clés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-ordering",
   "metadata": {},
   "source": [
    "Ce test unitaire à pour but de vérifier que la fonction Vectorisation renvoie un bien une DataFrame lorsqu'elle prend une phrase quelconque en argument. Le test verifiera aussi que les colonnes du DataFrame crée corresponde avec celui de Data_train passé en argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "undefined-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unitaire_vectorisation():\n",
    "    tweet = str(uuid.uuid4())\n",
    "    tweet = phrase.replace('-', ' ')\n",
    "    test_list = np.random.randint(10, size=(1,3))\n",
    "    Data_testunitaire = pd.DataFrame(test_list, columns=['column1', 'column2', 'column3'])\n",
    "    for i in range(10):\n",
    "        vectorisation = Vectorisation(Data_testunitaire, tweet)\n",
    "        if vectorisation.shape != Data_testunitaire.shape:\n",
    "            return False\n",
    "        if not (vectorisation.columns == Data_testunitaire.columns).all():\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "noble-latvia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_unitaire_vectorisation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
