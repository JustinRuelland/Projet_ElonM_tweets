{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "meaning-formula",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "important-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "substantial-reggae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Label_train_2.csv', '.DS_Store', 'Creation_matrice_Creation.ipynb', 'FinalDBelon.csv', 'Data_train.csv', 'DataNONEL_clean.csv', 'Clean_Tweets_EM.ipynb', 'Creation_matrice_Detection.ipynb', 'README.md', 'DataBase_temporary_creation.csv', 'Tweets_Not_ElonMusk.csv', 'DataBase.csv', '.gitignore', 'Reseau_neurone.ipynb', 'Data_train_2.csv', 'DataElon_clean.csv', 'Vectorisation.ipynb', '.ipynb_checkpoints', 'Label_train.csv', 'Clean_Tweets_NONEM.ipynb', '.git', 'TweetsElonMusk.csv']\n"
     ]
    }
   ],
   "source": [
    "arr = os.listdir('.')\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "realistic-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hasattr(tf, 'function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "finished-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train = pd.read_csv(\"Data_train.csv\")\n",
    "Label_train = pd.read_csv(\"Label_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "stone-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train_2 = pd.read_csv('Data_train_2.csv')\n",
    "Label_train_2 = pd.read_csv(\"Label_train_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "numerous-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Construction_modele(Data_train, Label_train):\n",
    "    #Construction couche de neurones\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))\n",
    "    #Mise au bon format des donnes pour le modele\n",
    "    Tweet_1 = Data_train.values\n",
    "    Tweet_2 = Tweet_1.astype(dtype='float32')\n",
    "    Label_1 = Label_train.values\n",
    "    Label_2 = Label_1[:,0].astype(dtype='uint8')\n",
    "    #Ajustement modele et choix de metrique\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "    #Entrainement du modele\n",
    "    model.fit(Tweet_2, Label_2, epochs=18)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "appreciated-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorisation(Data_train, tweet):   #prend un tweet en argument sous la forme d'une string et renvoi son vecteur sous forme d'une database dans l espace vectoriel de la matrice\n",
    "    Vecteurs = Data_train.columns\n",
    "    row = []\n",
    "    \n",
    "    dictionnary = dict()\n",
    "    for mot in Vecteurs:\n",
    "        dictionnary[mot] = 0\n",
    "    \n",
    "    Mot_tweet = tweet.split(\" \")\n",
    "    \n",
    "    for index, mot in enumerate(Mot_tweet):\n",
    "            if mot in Vecteurs:\n",
    "                dictionnary[mot] = index + 1\n",
    "                \n",
    "    row.append(dictionnary)\n",
    "    Data_out = DataFrame(row)\n",
    "    \n",
    "    if Data_out.loc[0].max() != 0:\n",
    "        Data_out.loc[0] = Data_out.loc[0] / Data_out.loc[0].max()\n",
    "    \n",
    "    return Data_out\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-salvation",
   "metadata": {},
   "source": [
    "La fonction detection prend en argument un tweet sous la forme d'un string, le reseau de neurone entraine et la base de donnee d entrainement utilise pour vectorizer le tweet. La fonction renvoi un tuple de la forme\n",
    "\\\n",
    "\\\n",
    "(\"Elon Musk detecte a x%\", 1)\n",
    "\\\n",
    "(\"Indetermine\", 2)\n",
    "\\\n",
    "(\"Pas Elon Musk a x%\", 0)\n",
    "\\\n",
    "\\\n",
    "1 est renvoye ssi x >= 60%\n",
    "\\\n",
    "0 est renvoye ssi x >= 60%\n",
    "\\\n",
    "2 est renvoye ssi la probabilite que ce soit Elon Musk (ou non) soit comprise entre 40% et 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "recorded-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(tweet_sentence, model_neurone, Data_train):\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    vecteur = vecteur_phrase.values\n",
    "    vecteur = vecteur.astype(dtype='float32')\n",
    "    if vecteur.sum() == 0:\n",
    "        char = \"Indetermine\"\n",
    "        return (char, 2)\n",
    "    Prediction = model_neurone.predict(vecteur)\n",
    "    if Prediction[0, 1] >= 0.6:\n",
    "        char = \"Elon Musk detecte a \" + str(Prediction[0, 1]) + \"%\"\n",
    "        return (char, 1)\n",
    "    elif Prediction[0,1] >= 0.4:\n",
    "        char = \"Indetermine\"\n",
    "        return (char, 2)\n",
    "    else :\n",
    "        char = \"Pas Elon Musk a \" + str(Prediction[0, 0]) + \"%\"\n",
    "        return (char, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "correct-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecte et rend une estimation du tweet propose et l ajoute a son entrainement\n",
    "#Renvoi lestimation et le nouveau model de neurone entraine apres estimation avec le tweet donne\n",
    "\n",
    "def detection_apprentissage(tweet_sentence, model_neurone, Data_train, Label_train, val=0):\n",
    "    #estimation\n",
    "    (char, val) = detection(tweet_sentence, model_neurone, Data_train)\n",
    "    #re entrainement du reseau de neurones\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    Data_train = pd.concat([Data_train, vecteur_phrase], ignore_index=True)\n",
    "    Data_train.reset_index(drop=True, inplace=True)\n",
    "    label = DataFrame([{'label':val}])\n",
    "    Label_train = pd.concat([Label_train, label], ignore_index=True)\n",
    "    Label_train.reset_index(drop=True, inplace=True)\n",
    "    model = Construction_modele(Data_train, Label_train)\n",
    "    return ((char, val), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fossil-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation des reseaux de neurones avant le dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "spare-toddler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.6860 - accuracy: 0.5658\n",
      "Epoch 2/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.6704 - accuracy: 0.6687\n",
      "Epoch 3/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.6446 - accuracy: 0.7273\n",
      "Epoch 4/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.6013 - accuracy: 0.7545\n",
      "Epoch 5/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.5424 - accuracy: 0.7765\n",
      "Epoch 6/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4816 - accuracy: 0.7952\n",
      "Epoch 7/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4315 - accuracy: 0.8178\n",
      "Epoch 8/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3936 - accuracy: 0.8341\n",
      "Epoch 9/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3648 - accuracy: 0.8468\n",
      "Epoch 10/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3417 - accuracy: 0.8543\n",
      "Epoch 11/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8638\n",
      "Epoch 12/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3097 - accuracy: 0.8681\n",
      "Epoch 13/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2968 - accuracy: 0.8717\n",
      "Epoch 14/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2868 - accuracy: 0.8775\n",
      "Epoch 15/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.8806\n",
      "Epoch 16/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2694 - accuracy: 0.8858\n",
      "Epoch 17/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2620 - accuracy: 0.8882\n",
      "Epoch 18/18\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2561 - accuracy: 0.8899\n"
     ]
    }
   ],
   "source": [
    "reseau_neurone_detection = Construction_modele(Data_train, Label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "formal-impossible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.6872 - accuracy: 0.5521\n",
      "Epoch 2/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.6754 - accuracy: 0.6146\n",
      "Epoch 3/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.6579 - accuracy: 0.6694\n",
      "Epoch 4/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.6314 - accuracy: 0.7017\n",
      "Epoch 5/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.5976 - accuracy: 0.7197\n",
      "Epoch 6/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.5625 - accuracy: 0.7385\n",
      "Epoch 7/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.5305 - accuracy: 0.7559\n",
      "Epoch 8/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.5031 - accuracy: 0.7691\n",
      "Epoch 9/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.4792 - accuracy: 0.7792\n",
      "Epoch 10/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7935\n",
      "Epoch 11/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.8038\n",
      "Epoch 12/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.4235 - accuracy: 0.8126\n",
      "Epoch 13/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.4098 - accuracy: 0.8214\n",
      "Epoch 14/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.3982 - accuracy: 0.8280\n",
      "Epoch 15/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3878 - accuracy: 0.8325\n",
      "Epoch 16/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.3779 - accuracy: 0.8341\n",
      "Epoch 17/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3704 - accuracy: 0.8398\n",
      "Epoch 18/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.3634 - accuracy: 0.8411\n"
     ]
    }
   ],
   "source": [
    "reseau_neurone_creation = Construction_modele(Data_train_2, Label_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "peaceful-tournament",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Pas Elon Musk a 0.8604091%', 0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'I sing every morning in my shower, i love '\n",
    "#reponse = detection(phrase, reseau_neurone_creation, Data_train_2)\n",
    "#reponse = detection_apprentissage(phrase, reseau_neurone_detection, Data_train, Label_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "brave-ensemble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((_, val), _) = reponse\n",
    "val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
