{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominant-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from cleantext import clean\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "saving-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incomplete-moderator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Label_train_2.csv', '.DS_Store', 'Creation_matrice_Creation.ipynb', 'FinalDBelon.csv', 'Data_train.csv', 'DataNONEL_clean.csv', 'Clean_Tweets_EM.ipynb', 'Creation_matrice_Detection.ipynb', 'README.md', 'DataBase_temporary_creation.csv', 'Tweets_Not_ElonMusk.csv', 'DataBase.csv', '.gitignore', 'Reseau_neurone.ipynb', 'Data_train_2.csv', 'DataElon_clean.csv', '.ipynb_checkpoints', 'Label_train.csv', 'Clean_Tweets_NONEM.ipynb', '.git', 'TweetsElonMusk.csv']\n"
     ]
    }
   ],
   "source": [
    "arr = os.listdir('.')\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worldwide-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hasattr(tf, 'function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "optical-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train = pd.read_csv(\"Data_train.csv\")\n",
    "Label_train = pd.read_csv(\"Label_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "concerned-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train_2 = pd.read_csv('Data_train_2.csv')\n",
    "Label_train_2 = pd.read_csv(\"Label_train_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "selected-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Construction_modele(Data_train, Label_train, affichage=1):\n",
    "    #Construction couche de neurones\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))\n",
    "    #Mise au bon format des donnes pour le modele\n",
    "    Tweet_1 = Data_train.values\n",
    "    Tweet_2 = Tweet_1.astype(dtype='float32')\n",
    "    Label_1 = Label_train.values\n",
    "    Label_2 = Label_1[:,0].astype(dtype='uint8')\n",
    "    #Ajustement modele et choix de metrique\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "    #Entrainement du modele\n",
    "    model.fit(Tweet_2, Label_2, epochs=18, verbose=affichage)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interesting-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorisation(Data_train, tweet):   #prend un tweet en argument sous la forme d'une string et renvoi son vecteur sous forme d'une database dans l espace vectoriel de la matrice\n",
    "    Vecteurs = Data_train.columns\n",
    "    row = []\n",
    "    \n",
    "    dictionnary = dict()\n",
    "    for mot in Vecteurs:\n",
    "        dictionnary[mot] = 0\n",
    "    \n",
    "    Mot_tweet = tweet.split(\" \")\n",
    "    \n",
    "    for index, mot in enumerate(Mot_tweet):\n",
    "            if mot in Vecteurs:\n",
    "                dictionnary[mot] = index + 1\n",
    "                \n",
    "    row.append(dictionnary)\n",
    "    Data_out = DataFrame(row)\n",
    "    \n",
    "    if Data_out.loc[0].max() != 0:\n",
    "        Data_out.loc[0] = Data_out.loc[0] / Data_out.loc[0].max()\n",
    "    \n",
    "    return Data_out\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anticipated-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(x):\n",
    "    if x.max() != 0:\n",
    "        x = x / x.max()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "magnetic-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorisation_liste(Data_train, liste_tweet, func_norma = normalisation):   \n",
    "    Vecteurs = Data_train.columns\n",
    "    \n",
    "    row = []\n",
    "    \n",
    "    dictionnary = dict()\n",
    "    for mot in Vecteurs:\n",
    "        dictionnary[mot] = 0\n",
    "    \n",
    "    for tweet in liste_tweet:\n",
    "        \n",
    "        Mot_tweet = tweet.split(\" \")\n",
    "        dictionnary2 = dictionnary.copy()\n",
    "        for index, mot in enumerate(Mot_tweet):\n",
    "            if mot in Vecteurs:\n",
    "                dictionnary2[mot] = index + 1\n",
    "                \n",
    "        row.append(dictionnary2)\n",
    "        \n",
    "    Data_out = DataFrame(row)\n",
    "    Data_out = Data_out.apply(normalisation, axis=1)\n",
    "        \n",
    "    return Data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-pollution",
   "metadata": {},
   "source": [
    "La fonction detection prend en argument un tweet sous la forme d'un string, le reseau de neurone entraine et la base de donnee d entrainement utilise pour vectorizer le tweet. La fonction renvoi un tuple de la forme\n",
    "\\\n",
    "\\\n",
    "(\"Elon Musk detecte a x%\", 1)\n",
    "\\\n",
    "(\"Indetermine\", 2)\n",
    "\\\n",
    "(\"Pas Elon Musk a x%\", 0)\n",
    "\\\n",
    "\\\n",
    "1 est renvoye ssi x >= 60%\n",
    "\\\n",
    "0 est renvoye ssi x >= 60%\n",
    "\\\n",
    "2 est renvoye ssi la probabilite que ce soit Elon Musk (ou non) soit comprise entre 40% et 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "regular-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(tweet_sentence, model_neurone, Data_train):\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    vecteur = vecteur_phrase.values\n",
    "    vecteur = vecteur.astype(dtype='float32')\n",
    "    if vecteur.sum() == 0:\n",
    "        char = \"Indetermine\"\n",
    "        return (char, 2)\n",
    "    Prediction = model_neurone.predict(vecteur)\n",
    "    if Prediction[0, 1] >= 0.6:\n",
    "        char = \"Elon Musk detecte a \" + str(Prediction[0, 1]) + \"%\"\n",
    "        return (char, 1)\n",
    "    elif Prediction[0,1] >= 0.4:\n",
    "        char = \"Indetermine\"\n",
    "        return (char, 2)\n",
    "    else :\n",
    "        char = \"Pas Elon Musk a \" + str(Prediction[0, 0]) + \"%\"\n",
    "        return (char, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "administrative-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_creation(tweet_sentence, model_neurone, Data_train):\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    vecteur = vecteur_phrase.values\n",
    "    vecteur = vecteur.astype(dtype='float32')\n",
    "    if vecteur.sum() == 0:\n",
    "        return 0\n",
    "    Prediction = model_neurone.predict(vecteur)\n",
    "    return Prediction[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "useful-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecte et rend une estimation du tweet propose et l ajoute a son entrainement\n",
    "#Renvoi lestimation et le nouveau model de neurone entraine apres estimation avec le tweet donne\n",
    "\n",
    "def detection_apprentissage(tweet_sentence, model_neurone, Data_train, Label_train, val=0):\n",
    "    #estimation\n",
    "    (char, val) = detection(tweet_sentence, model_neurone, Data_train)\n",
    "    #re entrainement du reseau de neurones\n",
    "    tweet_sentence = tweet_sentence.lower()\n",
    "    tweet_sentence = clean(tweet_sentence, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    vecteur_phrase = Vectorisation(Data_train, tweet_sentence)\n",
    "    Data_train = pd.concat([Data_train, vecteur_phrase], ignore_index=True)\n",
    "    Data_train.reset_index(drop=True, inplace=True)\n",
    "    label = DataFrame([{'label':val}])\n",
    "    Label_train = pd.concat([Label_train, label], ignore_index=True)\n",
    "    Label_train.reset_index(drop=True, inplace=True)\n",
    "    model = Construction_modele(Data_train, Label_train, affichage=0)\n",
    "    return ((char, val), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "retired-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prend en argument une liste de tweet sous la forme [\"tweet1\", \"tweet2\"…] et les labels associes sous la forme dune liste [0,0…] et renvoi une liste [estimation, model, Data_train, Label_train] où estimation est une liste des estimation faite de la liste de tweets passe, model est le reseau de neurones reentraines sur la liste de tweet apres avoir estimer et Data_train Et Label_train sont une concatenation des datasets donne et ceux utilise a exante \n",
    "def detection_apprentissage_liste(tweet_liste, model_neurone, Data_train, Label_train, val_list):\n",
    "    Estimat = []\n",
    "    Data_adding = Vectorisation_liste(Data_train, tweet_liste)\n",
    "    Data = Data_train.copy()\n",
    "    \n",
    "    Label_adding = pd.DataFrame(val_list, columns=['label'])\n",
    "    Label = Label_train.copy()\n",
    "    \n",
    "    Data_add = Data_adding.values\n",
    "    Data_add = Data_add.astype(dtype='float32')\n",
    "    \n",
    "    valued = model_neurone.predict(Data_add)\n",
    "    \n",
    "    Data = pd.concat([Data, Data_adding], ignore_index = True)\n",
    "    Label = pd.concat([Label, Label_adding], ignore_index = True)\n",
    "    \n",
    "    model_neurone = Construction_modele(Data, Label, affichage=0)\n",
    "    \n",
    "    valeur = (valued[:,1] > 0.5).astype(dtype='int').tolist()\n",
    "\n",
    "    \n",
    "    return [model_neurone, Data, Label, tweet_liste, valeur]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fancy-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = ['test1', 'test2', 'space fake', 'fake news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model, Data, Label, tweetl, val] = detection_apprentissage_liste(tweet_list, reseau_neurone_detection, Data_train, Label_train, [0,0,0,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection('i love my fan', model, Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation des reseaux de neurones avant le dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "reliable-silence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.6873 - accuracy: 0.5474\n",
      "Epoch 2/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.6671 - accuracy: 0.6642\n",
      "Epoch 3/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.6351 - accuracy: 0.7184\n",
      "Epoch 4/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.5861 - accuracy: 0.7522\n",
      "Epoch 5/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.5257 - accuracy: 0.7779\n",
      "Epoch 6/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.8031\n",
      "Epoch 7/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8223\n",
      "Epoch 8/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3888 - accuracy: 0.8376\n",
      "Epoch 9/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3619 - accuracy: 0.8469\n",
      "Epoch 10/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.8531\n",
      "Epoch 11/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3246 - accuracy: 0.8603\n",
      "Epoch 12/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3111 - accuracy: 0.8681\n",
      "Epoch 13/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2988 - accuracy: 0.8718\n",
      "Epoch 14/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2890 - accuracy: 0.8753\n",
      "Epoch 15/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2797 - accuracy: 0.8806\n",
      "Epoch 16/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2720 - accuracy: 0.8842\n",
      "Epoch 17/18\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8881\n",
      "Epoch 18/18\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.2587 - accuracy: 0.8862\n"
     ]
    }
   ],
   "source": [
    "reseau_neurone_detection = Construction_modele(Data_train, Label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "overall-exposure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.6897 - accuracy: 0.5377\n",
      "Epoch 2/18\n",
      "370/370 [==============================] - 0s 1ms/step - loss: 0.6817 - accuracy: 0.5887\n",
      "Epoch 3/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.6713 - accuracy: 0.6398\n",
      "Epoch 4/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.6553 - accuracy: 0.6750\n",
      "Epoch 5/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.6312 - accuracy: 0.7016\n",
      "Epoch 6/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.6000 - accuracy: 0.7181\n",
      "Epoch 7/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.5662 - accuracy: 0.7400\n",
      "Epoch 8/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.5347 - accuracy: 0.7575\n",
      "Epoch 9/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.5073 - accuracy: 0.7700\n",
      "Epoch 10/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.4836 - accuracy: 0.7856\n",
      "Epoch 11/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.4630 - accuracy: 0.7927\n",
      "Epoch 12/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.4442 - accuracy: 0.8024\n",
      "Epoch 13/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.4282 - accuracy: 0.8125\n",
      "Epoch 14/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.4143 - accuracy: 0.8177\n",
      "Epoch 15/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.4019 - accuracy: 0.8266\n",
      "Epoch 16/18\n",
      "370/370 [==============================] - 1s 1ms/step - loss: 0.3910 - accuracy: 0.8311\n",
      "Epoch 17/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3815 - accuracy: 0.8329\n",
      "Epoch 18/18\n",
      "370/370 [==============================] - 1s 2ms/step - loss: 0.3736 - accuracy: 0.8384\n"
     ]
    }
   ],
   "source": [
    "reseau_neurone_creation = Construction_modele(Data_train_2, Label_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debut du dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "printable-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Creation_tweet(nombre_mots_total, phrase, Data_train_2, model_neurone): #renvoi un tweet sous la forme d une phrase \n",
    "    phrase = phrase.lower()\n",
    "    phrase = clean(phrase, no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    liste_mot =  phrase.split(\" \")\n",
    "    if nombre_mots_total < len(liste_mot):\n",
    "        print(\"Erreur liste mot plus longue que le tweet prevu, longueur du tweet pris par defaut au nombre de mots\")\n",
    "        nombre_mots_total = len(liste_mot)\n",
    "    nombre_mot_rest = nombre_mots_total - len(liste_mot)\n",
    "    \n",
    "    liste_tweet = liste_mot + rnd.sample(list(Data_train_2.columns), nombre_mot_rest, counts=None)\n",
    "    tweet_pertinent = ' '.join(liste_tweet)\n",
    "    pertinence = detection_creation(tweet_pertinent, model_neurone, Data_train_2)\n",
    "    \n",
    "    for iteration in range(100):\n",
    "        if pertinence > 0.9:\n",
    "            tweet_pertinent = clean(tweet_pertinent, no_punct=True)\n",
    "            return tweet_pertinent\n",
    "        liste = liste_mot + rnd.sample(list(Data_train_2.columns), nombre_mot_rest, counts=None)\n",
    "        rnd.shuffle(liste)\n",
    "        current_tweet = ' '.join(liste)\n",
    "        current_pertinence = detection_creation(current_tweet, model_neurone, Data_train_2)\n",
    "        if current_pertinence > pertinence:\n",
    "            pertinence = current_pertinence\n",
    "            tweet_pertinent = current_tweet\n",
    "    current = clean(current_tweet, no_punct=True)\n",
    "    return current\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dress-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetos = Creation_tweet(8, '', Data_train_2, reseau_neurone_creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "compliant-roommate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very then needed start break trip plan'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection(tweetos, reseau_neurone_creation, Data_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pharmaceutical-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train_Detection = Data_train.copy()\n",
    "Data_train_Creation = Data_train_2.copy()\n",
    "\n",
    "Label_train_Detection = Label_train.copy()\n",
    "Label_train_Creation = Label_train_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "disciplinary-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue(iteration, nombre_mot ,reseau_neurone_detection, reseau_neurone_creation, Data_train, Data_train_2, Label_train, Label_train_2):\n",
    "\n",
    "    for i in range(iteration):\n",
    "        \n",
    "        #Creation parle en premier et propose 30 tweets, qu il juge pertinent, a Detection\n",
    "        print('Tweet propose par creation a la ', i , 'eme iteration : \\n')\n",
    "        tweet_liste = []\n",
    "        valeur = [0]*30 \n",
    "        for j in range(30):\n",
    "            \n",
    "            tweet_proposal = Creation_tweet(nombre_mot, '', Data_train_2, reseau_neurone_creation)\n",
    "            print(tweet_proposal)\n",
    "            tweet_liste.append(tweet_proposal)\n",
    "        print('\\n')\n",
    "        \n",
    "        #Detection recoit ces 30 tweets, les evalues, les appreds en tenant compte qu ils sont faux, et retourne a creation ce qu il en pensait avant d apprendre qu ils etaient faux\n",
    "        \n",
    "        [reseau_neurone_detection, Data_train, Label_train, tweet_liste, evaluation] = detection_apprentissage_liste(tweet_liste, reseau_neurone_detection, Data_train, Label_train, valeur)\n",
    "        \n",
    "        #Affichage de ce que globalement detection en pensait\n",
    "        \n",
    "        print('Sur ces tweets propose detection en a trouve :', sum(evaluation), ' correct selon lui')\n",
    "        \n",
    "        #Creation apprend de ses tweets en les labelisant suivant ce que detection en avait pense a priori\n",
    "        \n",
    "        [reseau_neurone_creation, Data_train_2, Label_train_2, _, _] = detection_apprentissage_liste(tweet_liste, reseau_neurone_creation, Data_train_2, Label_train_2, evaluation)\n",
    "        \n",
    "        print('\\n \\n')\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dried-eagle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet propose par creation a la  0 eme iteration : \n",
      "\n",
      "shot where lets company tweets fair definitely\n",
      "any park ~ especially monday die problem\n",
      "slow off few didnt that listening climate\n",
      "please why cover yes let california problem\n",
      "best date side vevo weve sun software\n",
      "tell taking post tough exactly battery tomorrow\n",
      "rolling review build works close supercharger prob\n",
      "final collection asia production risk per build\n",
      "hw already fashion god vegas yes covered\n",
      "a water pass because autopilot body sign\n",
      "need aint pressure night california once spacex\n",
      "tell minimum shop yeah americans asked engineering\n",
      "lol if quality stream after launch idea\n",
      "us orbit call tunnel old software cause\n",
      "totally list advanced making rear best ai\n",
      "same during launch month obamacare may story\n",
      "moon seem question found asked tesla takes\n",
      "part fair unless in address obamacare lower\n",
      "lucky hard think test production given london\n",
      "friday velocity by america teslamotors earth isnt\n",
      "giant london minimum reality covered + force\n",
      "wouldnt thinking next start any number would\n",
      "especially am youre team feel yes economy\n",
      "obamacare hands doing even an happens dragon\n",
      "sent didnt auto truck engine an deep\n",
      "everything worlds supporting public president park plan\n",
      "amazing though action against team product science\n",
      "note artpop tweet obama jobs should amazing\n",
      "park thinking kind vehicle cool interesting americans\n",
      "great coverage china gigafactory humanity think makes\n",
      "\n",
      "\n",
      "Sur ces tweets propose detection en a trouve : 24  correct selon lui\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialogue(1, 8, reseau_neurone_detection, reseau_neurone_creation, Data_train_Detection, Data_train_Creation, Label_train_Detection, Label_train_Creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-costa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-arabic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-gates",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-appliance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-mauritius",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'I sing every morning in my shower, i love space and tesla'\n",
    "#reponse = detection(phrase, reseau_neurone_creation, Data_train_2)\n",
    "reponse = detection_apprentissage(phrase, reseau_neurone_detection, Data_train, Label_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "((_, val), _) = reponse\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-variance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
