{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "adjusted-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "binary-geology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FinalDBelon.csv', 'Data_train.csv', 'DataNONEL_clean.csv', 'Clean_Tweets_EM.ipynb', 'Creation_matrice_Detection.ipynb', 'Création_matrice_Création.ipynb', 'README.md', 'Tweets_Not_ElonMusk.csv', 'DataBase.csv', '.gitignore', 'Reseau_neurone.ipynb', 'DataElon_clean.csv', 'Vectorisation.ipynb', '.ipynb_checkpoints', 'Label_train.csv', 'Clean_Tweets_NONEM.ipynb', '.git', 'TweetsElonMusk.csv']\n"
     ]
    }
   ],
   "source": [
    "arr = os.listdir('.')\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unauthorized-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataElon = pd.read_csv('DataElon_clean.csv')\n",
    "DataNONElon = pd.read_csv('DataNONEL_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf3df3",
   "metadata": {},
   "source": [
    "La fonction cleaner permet de \"nettoyer\" la base de données, afin de rendre les données \"traitables\" par notre réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "brave-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(Database_init):\n",
    "    Database = Database_init.copy()   \n",
    "    for i in Database.index:\n",
    "        Database.loc[i, 'tweet'] = str(Database.loc[i, 'tweet']).lower()\n",
    "        if '@' in str(Database.loc[i,\"tweet\"]):\n",
    "            Tweet = Database.loc[i, 'tweet'].split(' ')\n",
    "            for index, mot in enumerate(Tweet):\n",
    "                if '@' in mot:\n",
    "                    del Tweet[index]\n",
    "            Database.loc[i, 'tweet'] = ' '.join(Tweet)\n",
    "        \n",
    "        Database.loc[i, 'tweet'] = clean(Database.loc[i, 'tweet'], no_emoji=True, no_punct=True, no_urls = True, replace_with_url='', no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "        \n",
    "        \n",
    "    return Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e2e9f",
   "metadata": {},
   "source": [
    "Permet de créer la liste de tous les mots utilisés dans les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "integrated-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_mot(Liste):\n",
    "    liste = [' '.join(Liste)]\n",
    "    Liste_mots_doublon = liste[0].split(' ')\n",
    "    for index, mot in enumerate(Liste_mots_doublon):\n",
    "        if '@' in mot:\n",
    "            Liste_mots_doublon[index] = ''\n",
    "    Liste_mots = list(set(Liste_mots_doublon))\n",
    "    return Liste_mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce144c15",
   "metadata": {},
   "source": [
    "A partir des tweets et de la liste de tous les mots employés, creation_matrice permet de vectoriser en un Dataframe tous les tweets. Pour chaque tweet, on trouve dans la colonne correspondant à chaque mot sa place dans le tweet. Si un mot apparaît plusieurs fois, c'est sa dernière position qui est indiquée (le cas est relativement peu fréquent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "committed-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_matrice(Liste_mots, Liste):\n",
    "    row =[]\n",
    "    \n",
    "    dictionnary = dict()\n",
    "    for mot in Liste_mots:\n",
    "        dictionnary[mot] = 0\n",
    "    \n",
    "    for tweet in Liste:\n",
    "        dictionnary2 = dictionnary.copy()\n",
    "        tweet_actual = tweet.split(\" \")\n",
    "        tweet_actual_clean = []\n",
    "        for mot_tweet in tweet_actual:\n",
    "            if mot_tweet != '':\n",
    "                tweet_actual_clean.append(mot_tweet)\n",
    "        for index, mot in enumerate(tweet_actual_clean):\n",
    "            if mot in Liste_mots:\n",
    "                dictionnary2[mot] = index + 1 \n",
    "        row.append(dictionnary2)\n",
    "    return DataFrame(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c6203",
   "metadata": {},
   "source": [
    "Optimize_matrice permet de supprimer de la dataframe les mots qui apparaissent dans moins de n_min mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "subtle-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_matrice(Dataframe, n_min):\n",
    "    for col in Dataframe.columns:\n",
    "        if len(Dataframe[Dataframe[col] != 0]) < n_min:\n",
    "            Dataframe.drop(columns = col, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12273111",
   "metadata": {},
   "source": [
    "Normalize permet de transformer les positions \"absolues\" des mots dans les tweets en positions \"relatives\". Par exemple pour le tweet \"The sky is blue\", la colonne \"sky\" qui contenait 2, contiendra après un appel de normalize 1/2.\n",
    "Cela permet de ne pas fausser la partie d'apprentissage des réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "general-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(Matrix):\n",
    "    Matrix_out = Matrix.copy()\n",
    "    index = Matrix_out.index\n",
    "    for ind in index:\n",
    "        Matrix_out.loc[ind] = Matrix_out.loc[ind] / Matrix_out.loc[ind].max()\n",
    "    return Matrix_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b245b21",
   "metadata": {},
   "source": [
    "On applique ensuite ces fonctions aux données, pour obtenir le Dataframe nécessaire à l'élaboration du réseau de neurones \"Création\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataElon = cleaner(DataElon)\n",
    "Liste_tweets_EM = DataElon[\"tweet\"].values\n",
    "\n",
    "DataNONElon = cleaner(DataNONElon)\n",
    "DataNONElon.dropna(inplace=True)\n",
    "Liste_tweets_NONEM = DataNONElon[\"tweet\"].values\n",
    "\n",
    "Liste_tweets = np.concatenate((Liste_tweets_EM[:6000], Liste_tweets_NONEM[:6000]))\n",
    "Liste_mots = creation_mot(Liste_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pending-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataBase_EM = creation_matrice(Liste_mots, Liste_tweets_EM[:6000])\n",
    "DataBase_EM.drop(columns = '', axis=1, inplace=True)\n",
    "\n",
    "DataBase_NEM = creation_matrice(Liste_mots, Liste_tweets_NONEM[:6000])\n",
    "DataBase_NEM.drop(columns = '', axis=1, inplace=True)\n",
    "\n",
    "DataBase = pd.concat([DataBase_EM, DataBase_NEM], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_matrice(DataBase, 18)\n",
    "DataBase_word = DataBase.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rajoute une colonne label : 1 pour un Tweet d'Elon Musk, 0 sinon\n",
    "DataBase_word['label'] = (DataBase_word.index < 6000).astype(np.int64)\n",
    "\n",
    "#Supprime les Tweets qui ont des zéros pour toutes les variables\n",
    "L =[]\n",
    "for i in DataBase_word.index:\n",
    "    if DataBase_word.iloc[i, :-1].sum() == 0:\n",
    "        L.append(i)\n",
    "DataBase_word.drop(index = L, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "useful-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataBase_word.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "becoming-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrice_train = DataBase_word.iloc[:,:-1]\n",
    "Label_train = DataBase_word.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "selective-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTrain = normalize(Matrice_train)\n",
    "DataTrain.to_csv(\"Data_train.csv\", index=False)\n",
    "Label_train.to_csv(\"Label_train.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
